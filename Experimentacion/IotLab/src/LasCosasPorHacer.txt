<LISTO> - Armar red con un nodo organizador_de_red que siempre es el mismo y actuamos como que tiene siempre misma IPv6 harcodeada. 
	- En el caso que haya cambiado, nos vamos a dar cuenta rapido y es solo cambiar un valor en el codigo.
<LISTO> - Todos los nodos envian su ip al nodo organizador_de_red, ademas contandole cual rol cumplen (si worker o coordinator). 
<LISTO> - El nodo organizador_de_red crea la red a emular con todas las ips que necesita y luego publica a cada nodo su posicion en la red.
	<LISTO> - En un primer experimento todos los nodos estan conectados con el coordinador directamente.
	<LISTO> [Falta en simgrid tambien] - Los nodos pueden actuar de puente, conectando al coordinador con otros workers.
	[Deberia ser toggleable][Falta en simgrid tambien] - Si se recibe una task finalizada el coordinador le envia un ack al worker para que este sepa que no tiene que reenviarlo.
	<QUEDA PARA DESPUES> - El coordinador puede ser != del nodo 1

<LISTO> - El nodo coordinador, ya sabiendo cuantos nodos tiene a su disposicion (info recibida del organizador_de_red) y como es la tarea a resolver, separa en las particiones y envia la info.

- Desconexiones pensar con que grado de fidelidad implementarlas.
	<LISTO> - Si la desconexion tiene lugar cuando el mensaje deberia llegar al receptor, entonces no se recibe.
	<QUEDA PARA DESPUES> [Falta en simgrid tambien] - El coordinador reenvia tasks a nodos que no respondieron durante mucho tiempo de manera duplicada para ver si antes no estaban accesibles pero ahora si.
	<QUEDA PARA DESPUES> - Si la desconexion tiene lugar tampoco se pueden enviar mensajes
	<QUEDA PARA DESPUES> <POR AHI ES DEMASIADO REBUSCADO> - Si la desconexion tiene lugar durante el tiempo entre que se envia el mensaje y se recibe, entonces no se recibe.

<LISTO> - Existe una historia de desconexiones que se puede escribir en un .txt aparte y se envia a cada nodo su parte (diciendole cuando desconectarse y cuando volver).

<QUEDA PARA DESPUES?> - Antes de que el coordinador empiece a  correr el experimento espera a recibir un ack de todos los nodos worker diciendole que saben que son worker y estan empezando a actuar como su rol.

- El experimento se maneja igual que en simgrid hasta resolverse el MapReduce simulado.
	<LISTO> - Coordinador separa tasks y las envia a todos los nodos workers.
	<LISTO> - Workers "hacen maps" (operacion filler) y envian resultado al coordinador.
	<LISTO> - Coordinador reenvia cuando hay timeout.
	<LISTO> - Coordinador recibe maps, cuando tiene todos los maps "reduce" (operacion filler) y avisa que termino.
		<LISTO> - Cuando el coordinador updatea estado y reenvia tiene un mutex para que no hayan varios threads updateando al mismo tiempo.
	<LISTO> - Redundancia.
	<LISTO> - Threshold.

- El coordinador puede ser cualquier nodo, no necesariamente el organizador de la network.

<LISTO> - Una vez terminado el MapReduce, los nodos worker envian al coordinador la informacion relevante para el experimento y esta info queda guardada en un archivo en el coordinador.
<LISTO> - Logging

- Para interferencias podria hacer que linea con mucha interferencia pueda recibir/enviar mensajes con % de probabilidad segun su calidad de conexion. <Por ahora no>



- Cuanto tarda una iteracion en un A8.

- <LISTO PARA VERSION PRE OTRAS MEJORES> Cuanto tardan 11000 iteraciones en un unico nodo (walltime). (corriendo sin latencia este nodo). [Alrededor de 294130, vs 40674 en el distribuido con 30 nodos).

- <LISTO> Mandar tarea testigo ad hoc a todos los nodos al principio para ganar info sobre su performance, asumimos que la corre de vez en cuando. -cada cuanto la manda depende de la estabilidad performance de respuestas.

- Enviar binario con parametro iteraciones. Pesa 2 mb poruqe tengo que compilar usando static.
	Alternativa simple: Llenar de basura el mensaje que se envia hasta llegar a unos cuantos Kb que es lo que nos importa.
	Alternativa elegida por ahora: Compilar en el mismo programa antes de empezar el experimento.

		PREGUNTA, pensamos cada tarea indivudal como un binario separado, entonces si agrupo 10 tareas para un nodo tengo que agrupar 10 binarios y mandarselos uno por uno?
		Dado que es un map en realidad se podria ver como el mismo binario con distintos parametros, para evitar peso

		QUEDA: Mando un mensaje por cada tarea


- Identificar nodos, para filtrar los malos.

- <FALTA TESTEAR> Nodos apagados, no desconectados.

- <FALTA> Agregar un timeout al envio de tareas de prueba cosa que despues de x tiempo decida cortar con la espera y arrancar con los nodos que contestaron, guardando a los demas como nodos ocupados (excluidos) por ahora.


- <LISTO> compilar y enviar binarios
- <FALTA TESTEAR> Nodos apagados, no desconectados. Testeando


- problemas, reunion: 
	- Cual es el punto de seguir usando redundancia por grupos (asigno un grupo de tareas a cada nodo y envio un cacho de cada grupo al resto de los nodos).
	 Se pierde un poco el significado de asignar un grupo a un nodo si de todos modos cada tarea se envia por separado.
	 Una solucion: envio una rafaga de mensajes, como para imitar lo de enviar el grupo de una en un solo mensaje como hacia hasta ahora.
					// Mensajes individuales causan un problema con el tema de la redundancia por grupos de tareas.
	
	// Pregunta medio para el futuro
	- Mando tarea testigo, si algunos nodos no responden los dejo de usar (hasta que consiga alguna respuesta de ellos). 

	Separo a los nodos en grupos, la idea es que estos grupos se mantengan a lo largo del tiempo?

Redundancia: 
	- Replicar tareas y enviarselas a varios nodos / Agruparlas y usar redundancia raid segun la relacion #nodos/#tareas. Si estoy agrupando tareas esta bien enviar mas de una tarea en un mensaje.

	- redundancia compartida por grupos: Redundancia tipo raid
	- redundancia individual por tareas: Redundancia tratando de distribuir de una en una todas las tareas y ocupar los nodos lo mas posible

	- Criticidad == Calidad de servicio que se quiere determina cuanta redundancia y cuanto uso de nodos hago, segun gasto.
		- Esperar al timeout/No esperar al timeout.
		- Seleccionar calidad de nodos segun criticidad (?).




- <FALTA TESTEAR> Se envian benchmarks cada x segundos
	- <QUEDA PARA DESPUES> Si el nodo está muy ocupado en el momento (muy cargado de tareas), se guarda como benchmark pendiente y cada vez que se recibe un resultado de ese nodo se reevalúa si es posible enviar el benchmark.

- MapReduce llega mas tarde, el nodo esta preparado pero no arranca hasta que le llega el mapreduce (tareas con la # de iteraciones que toma cada una).

- Puede llegar mas de un MapReduce y los ejecute en orden, termina un MapReduce y arranca otro.
	- Los MapReduce pueden ser ejecutados concurrentemente.
		Ideas:
			- Cada tarea sigue siendo independiente, pero guarda mas info/ Tengo PendingMapReduce con todas las tasks.
			- Para saber si ya termino un MapReduce voy a necesitar el PendingMapReduce seguro.
				- Puedo tener un timeout para las tasks seteado por el usuario para cada PendingMapReduce.

- Pensar una manera mas copada de hacer el resend, para que tenga en cuenta criticidad
	- Implementar una/muchas politicas de revision de todos los pending map reduce para seleccionar tareas a enviar y a que nodos.


- <No estoy seguro de como meter esto, porque ahora la politica de resend se elige para garantizar que se reenvien todas las tareas, pero preservar el splice por si es necesario mas tarde> Resend reordena las tareas internamente en el PendingMapReduce segun cuales fueran reenviadas y cuales no como se hacia antes cuando no existia PendingMapReduce.


- DECISION DE BENCHMARKING: El coordinador deberia elegir como quiere manejar cuando un nodo tiene el benchmark sin contestar, que prioridad darle en comparacion con uno que ya respondio el benchmark pero tiene benchmark peor que el benchmark vencido del nodo que no respondio todavia. Quizas esperar con un cierto timeout a que responda algun mensaje, sea benchmark o otro tarea y si no respondio lo saca del pool hasta que responda? Y una vez que responde pasa a una fase de test en la que solo se le manda un benchmark antes de devolverlo al pool de nodos
				- PREGUNTA: CUANDO DECIDO REENVIAR LA TAREA DE BENCHMARK??? SI ESTA YA OCUPADO CON OTRAS TAREAS POR AHI SE COMPLICA MAS EL ASUNTO AL PEDO

AHORA: 
	- Decidir como asignar criticalidad a diferentes tareas y como manejar tareas de distinta criticalidad. Como evitar inanicion?

		- N = # de MapReduces activos.
		- Criticidad1..N = {1, 2, 3}
		- Prioridad_i = Criticidad_i * Tmpo_de_vida_i / #Tareas_pendientes_i

		- Aparte de esto, si un MR ya ligo un resend, darle prioridad a otro MR
			- Idea: Reevaluo cual es el proximo MR cada vez que me llega una tarea terminada, y ahi chequeo segun cuantas tareas queden y cuantos nodos hayan libres si sigo esperando para resendear o mando de una con la cantidad de nodos libres que haya.

			- Idea distinta: en vez de contar los resend de todo el MR, mejor tomar las tareas asignadas a N cantidad de nodos de cada MapReduce, asi tareas que ya estan asignadas a varios nodos no ligan un resend porque una que fue perdida en varios necesita ser reenviada.

		- Idea: Esperar a acumular threshold % nodos libres antes de triggerear un resend?

		- threshold: Uso el threshold para habilitar mapreduces/deshabilitarlos para ser enviados, pero esto podria producir inanicion en mapreduces que no estan siendo enviados, como juegan/se balancean el threshold y la prioridad? 
			- Uso el threshold y el timeout, si se triggerea el timeout primero lo agrego al pool de mapreduces disponibles para ser reenviados asi nomá y yafu.
			 ----> un timeout para cada uno.


	- Cambiar la forma en que el coordinador envia tareas a los nodos worker para que sea mas realista, nodos manejan una tarea a la vez (en vez de un grupo entero a la vez) y una vez que el coordinador recibe un resultado de una tarea la cancela en todos los otros nodos que la tengan asignada

	- Pasar la corrida de un experimento de ser un MapReduce y nada mas a estar escuchando pedidos de MapReduce y manejarlos a medida que le llegan
		- Enviar tareas testigos de manera periodica.
		- Manejar muchos MapReduces al mismo tiempo
			- Manejar criticidad evitando inanicion y tratando de terminar los mapreduces mas cerca de ser terminados  <--- 
				- Mire un poco de papers, deberia basarme en alguna cosa o probar armarme algo simple? Ya tengo pensado algo
			- Balancearlos para ver que elijo teniendo en cuenta criticidad y las otras cosas, pero tambien ver como usar el threshold
				- Pensar como integrar threshold a esto, esperar a juntar nodos o enviar a los que haya disponibles? 
				- Y ademas tener en cuenta si ya fue reenviada hace poco


				- Cada map reduce tiene su propio valor de threshold (por ahora es siempre el mismo valor), 
					- una vez que reenvió todas sus tareas a los nodos disponibles:
						- lo quito de la cola de MRs activos y 
						- lo paso a la cola de MRs pendientes pasivos,
						- además de esto, arranca un contador del timeout de este MR
					- cada vez que recibe una tarea completada de este MR:
						- chequea el threshold
							- si ya lo alcanzó 
								- vuelve a agregarlo a la cola de MRs pendientes activos 
								- lo quita de los pasivos
								- resetea el timeout

					- cuando se triggerea el timeout 
						- vuelve a agregarlo a la cola de MRs pendientes activos 
							- lo quita de los pasivos
							- resetea el timeout

				- Cada map reduce tiene su propio valor de timeout
					- Cuando envia todas las tareas a los nodos disponibles:
						-

					- si no alcanzo el threshold pero no hay otros MRs libres podria hacer que reenvie igual quizas (esto quizas solo con criticidad bien alta)


				- FALTA UN MECANISMO PARA DARME CUENTA CUANDO UN NODO ESTA MUERTO (algun tipo de timeout, no necesariamente las tareas testigo)

				- PREGUNTA: LOS NODOS WORKER DEBERIAN TENER UN MAXIMO DE CUANTAS TAREAS LES PUEDO ENVIAR AL MISMO TIEMPO, ESE MAXIMO DEBERIA SER REGULADO DEL LADO DEL COORDINADOR O DEL WORKER? TIENE MAS SENTIDO REGULARLA DEL LADO DEL COORDINADOR ME PARECE. Razon para que tengan el maximo: Mejor que no tengan tiempo idle si se puede evitar. Mantener la cola de tareas pendientes en cada worker llena.
					- setear un maximo de tareas para cada nodo y decidir de acuerdo a eso, no a si esta idle o no