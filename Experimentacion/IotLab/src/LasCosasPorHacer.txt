<LISTO> - Armar red con un nodo organizador_de_red que siempre es el mismo y actuamos como que tiene siempre misma IPv6 harcodeada. 
	- En el caso que haya cambiado, nos vamos a dar cuenta rapido y es solo cambiar un valor en el codigo.
<LISTO> - Todos los nodos envian su ip al nodo organizador_de_red, ademas contandole cual rol cumplen (si worker o coordinator). 
<LISTO> - El nodo organizador_de_red crea la red a emular con todas las ips que necesita y luego publica a cada nodo su posicion en la red.
	<LISTO> - En un primer experimento todos los nodos estan conectados con el coordinador directamente.
	<LISTO> [Falta en simgrid tambien] - Los nodos pueden actuar de puente, conectando al coordinador con otros workers.
	[Deberia ser toggleable][Falta en simgrid tambien] - Si se recibe una task finalizada el coordinador le envia un ack al worker para que este sepa que no tiene que reenviarlo.
	<QUEDA PARA DESPUES> - El coordinador puede ser != del nodo 1

<LISTO> - El nodo coordinador, ya sabiendo cuantos nodos tiene a su disposicion (info recibida del organizador_de_red) y como es la tarea a resolver, separa en las particiones y envia la info.

- Desconexiones pensar con que grado de fidelidad implementarlas.
	<LISTO> - Si la desconexion tiene lugar cuando el mensaje deberia llegar al receptor, entonces no se recibe.
	<QUEDA PARA DESPUES> [Falta en simgrid tambien] - El coordinador reenvia tasks a nodos que no respondieron durante mucho tiempo de manera duplicada para ver si antes no estaban accesibles pero ahora si.
	<QUEDA PARA DESPUES> - Si la desconexion tiene lugar tampoco se pueden enviar mensajes
	<QUEDA PARA DESPUES> <POR AHI ES DEMASIADO REBUSCADO> - Si la desconexion tiene lugar durante el tiempo entre que se envia el mensaje y se recibe, entonces no se recibe.

<LISTO> - Existe una historia de desconexiones que se puede escribir en un .txt aparte y se envia a cada nodo su parte (diciendole cuando desconectarse y cuando volver).

<QUEDA PARA DESPUES?> - Antes de que el coordinador empiece a  correr el experimento espera a recibir un ack de todos los nodos worker diciendole que saben que son worker y estan empezando a actuar como su rol.

- El experimento se maneja igual que en simgrid hasta resolverse el MapReduce simulado.
	<LISTO> - Coordinador separa tasks y las envia a todos los nodos workers.
	<LISTO> - Workers "hacen maps" (operacion filler) y envian resultado al coordinador.
	<LISTO> - Coordinador reenvia cuando hay timeout.
	<LISTO> - Coordinador recibe maps, cuando tiene todos los maps "reduce" (operacion filler) y avisa que termino.
		<LISTO> - Cuando el coordinador updatea estado y reenvia tiene un mutex para que no hayan varios threads updateando al mismo tiempo.
	<LISTO> - Redundancia.
	<LISTO> - Threshold.

- El coordinador puede ser cualquier nodo, no necesariamente el organizador de la network.

<LISTO> - Una vez terminado el MapReduce, los nodos worker envian al coordinador la informacion relevante para el experimento y esta info queda guardada en un archivo en el coordinador.
<LISTO> - Logging

- Para interferencias podria hacer que linea con mucha interferencia pueda recibir/enviar mensajes con % de probabilidad segun su calidad de conexion. <Por ahora no>



- Cuanto tarda una iteracion en un A8.

- <LISTO PARA VERSION PRE OTRAS MEJORES> Cuanto tardan 11000 iteraciones en un unico nodo (walltime). (corriendo sin latencia este nodo). [Alrededor de 294130, vs 40674 en el distribuido con 30 nodos).

- <LISTO> Mandar tarea testigo ad hoc a todos los nodos al principio para ganar info sobre su performance, asumimos que la corre de vez en cuando. -cada cuanto la manda depende de la estabilidad performance de respuestas.

- Enviar binario con parametro iteraciones. Pesa 2 mb poruqe tengo que compilar usando static.
	Alternativa simple: Llenar de basura el mensaje que se envia hasta llegar a unos cuantos Kb que es lo que nos importa.
	Alternativa elegida por ahora: Compilar en el mismo programa antes de empezar el experimento.

		PREGUNTA, pensamos cada tarea indivudal como un binario separado, entonces si agrupo 10 tareas para un nodo tengo que agrupar 10 binarios y mandarselos uno por uno?
		Dado que es un map en realidad se podria ver como el mismo binario con distintos parametros, para evitar peso

		QUEDA: Mando un mensaje por cada tarea


- Identificar nodos, para filtrar los malos.

- <FALTA TESTEAR> Nodos apagados, no desconectados.

- <FALTA> Agregar un timeout al envio de tareas de prueba cosa que despues de x tiempo decida cortar con la espera y arrancar con los nodos que contestaron, guardando a los demas como nodos ocupados (excluidos) por ahora.


- <LISTO> compilar y enviar binarios
- <FALTA TESTEAR> Nodos apagados, no desconectados. Testeando


- problemas, reunion: 
	- Cual es el punto de seguir usando redundancia por grupos (asigno un grupo de tareas a cada nodo y envio un cacho de cada grupo al resto de los nodos).
	 Se pierde un poco el significado de asignar un grupo a un nodo si de todos modos cada tarea se envia por separado.
	 Una solucion: envio una rafaga de mensajes, como para imitar lo de enviar el grupo de una en un solo mensaje como hacia hasta ahora.
					// Mensajes individuales causan un problema con el tema de la redundancia por grupos de tareas.
	
	// Pregunta medio para el futuro
	- Mando tarea testigo, si algunos nodos no responden los dejo de usar (hasta que consiga alguna respuesta de ellos). 

	Separo a los nodos en grupos, la idea es que estos grupos se mantengan a lo largo del tiempo?

Redundancia: 
	- Replicar tareas y enviarselas a varios nodos / Agruparlas y usar redundancia raid segun la relacion #nodos/#tareas. Si estoy agrupando tareas esta bien enviar mas de una tarea en un mensaje.

	- redundancia compartida por grupos: Redundancia tipo raid
	- redundancia individual por tareas: Redundancia tratando de distribuir de una en una todas las tareas y ocupar los nodos lo mas posible

	- Criticidad == Calidad de servicio que se quiere determina cuanta redundancia y cuanto uso de nodos hago, segun gasto.
		- Esperar al timeout/No esperar al timeout.
		- Seleccionar calidad de nodos segun criticidad (?).